In some cases we need to process data that arrives as a **continuous stream**, this happens because:
- The data is generated by some possibly infinite _evolving process_ (e.g. sensor readings)
- The data is _too large_ to be accessed in a random fashion, since in most data storage supports sequential access is (much) faster than random.

Hence we want to develop techniques that allow us to perform _data analysis_ **on the fly**, using _limited memory_, and _without having to store data_ for subsequent offline processing.

The typical data analysis tasks performed on such streams are:
- **Identification of frequent items**
- **Statistics collection**
- **Optimization and graph problems** (e.g. clustering)

### Streaming Model

The model we will use is of a _sequential machine_ with limited amount of memory that operates on a _continuous (one-way) stream_.

![[streaming-model.png]]

More formally:
Let $\Sigma = x_{1}, \dots, x_{n}, \dots$ denote the _input stream_ received sequentially, where $x_{i}$ is the $i$-th element received. Upon receiving $x_{n}$:
- Suitable data structures stored in the working memory are updated (**UPDATE task**)
- If required, a _solution_ for the problem at hand, relative to the input set $x_{1}, \dots, x_{n}$ is computed from the data stored in the working memory (**QUERY task**)

#### Key Performance Indicators

The main performance metrics considered are:
1. The size of the _working memory_ $s$. We aim to use much less memory compared to the size of the stream: $s \ll |\Sigma|$
2. The number of _sequential passes_ $p$ over the input stream. The aim is to only require a single pass, since the length of the stream is potentially infinite, or very long.
3. The _update time_ per item, i.e. the time complexity of updating the summarized representation with a new data point. The goal is to perform the update in constant time: $T_{u} = O(1)$.
4. The _query time_ $T_{q}$ to return a solution after seeing $x_{1}, \dots, x_{n}$ samples. The aim is for it to be independent from $n$.

Hence we need to find a way to both _summarize_ the informations given by the stream samples incrementally via a _synopsis data structure_, so that we can perform the desired analysis without storing the entire history; and to find suitable _approximate algorithms_ that can operate only looking at the data once.

As always we aim to find a suitable tradeoff between accuracy, working memory, and processing time per item.


### Majority Element

One of the simplest analysis that could be carried out on a data stream, would be to find out the _majority element_, that is, given a stream $\Sigma = x_{1}, .., x_{n}$, return the element $x$ (if any) that occurs $> \frac{n}{2}$ times in $\Sigma$.

In a standard setting it would be trivial to perform this task with an hashmap and a linear scan over the stream in $O(n)$ expected time.

But in a streaming setting we can't store the entire sequence in memory and perform the computation because it is potentially too large. Even updating the hashmap incrementally would not work, since all the elements may be different and we would run out of memory.

#### Boyer-Moore Algorithm

A very simple and elegant streaming algorithm that computes the majority element of a stream is the _Boyer-Moore Algorithm_.

The idea is for each new sample to either increment a counter if it corresponds to the current candidate, or to decrease the counter if it is different. If the counter is zero then the new sample becomes the new candidate.
If an element appears more than $\frac{n}{2}$ times it will "survive" all the decrements from the other elements, since they will happen less then half the time.

If no majority element exists then the algorithm returns an arbitrary value.

```pseudo
\begin{algorithm}
\caption{Boyer-Moore majority element algorithm}
\begin{algorithmic}
\State $cand \gets Null$
\State $count \gets 0$
\ForAll{$x_t \in \Sigma$}
	\If{$count = 0$}
		\State $cand \gets x_t$
		\State $count \gets count + 1$
	\Else
		\If{$cand = x_t$}
			\State $count \gets count + 1$
		\Else
			\State $count \gets count - 1$
        \EndIf
    \EndIf
\EndFor
\Return cand
\end{algorithmic}
\end{algorithm}
```

![[streaming-br-exa1.png | center]]
![[streaming-br-exa2.png | center]]

##### Analysis

Given a stream $\Sigma$ which contains a majority element $m$, the Boyer-Moore algorithm returns $m$ using:
- working memory of size $O(1)$, since the only state it keeps are the $cand$ and $count$ variables.
- 1 pass, obvious from construction
- $O(1)$ update and query times, again clear from the construction of the algorithm

Let's prove the correctness of the algorithm:

Let $cand_{t}$ and $count_{t}$ be the value of the variables after processing the $t$ sample.
By induction we can show that for each $t \geq{0}$, the substream $x_{1}, \dots, x_{t}$ can be partitioned into
- $count_{t} \geq 0$ instances of $m$
- $\frac{t-count_{t}}{2}$ instances of pairs $(e_{1}, e_{2})$, with $e_{1} \neq e_{2}$.

**Base case (t = 0)**: for $t = 0$ the stream is empty and no element has been processed, hence the substream $= \emptyset$ will be partitioned into:
- $0$ instances of $m = \emptyset$
- $0$ instances of pairs $= \emptyset$
which is true.

**Inductive case**: suppose that the inductive hypothesis is true, and let's add a new sample $x_{t+1}$, then either:
- $x_{t+1} = m \implies$ we can add it to the first partition. We obtain $count_{t+1} = count_{t}+1$ instances of $m$, and $\frac{t+1 - count_{t + 1}}{2} = \frac{t - count_{t}}{2}$ pairs.
- $x_{t+1} \neq m \implies$ we have to decrease the count: $count_{t+1} = count_{t} -1$. We can then "pop" an instance of $m$ in the first partition at $t$ and create a new pair $(x_{t+1}, m)$. Thus the size of the first partition is $count_{t+1} = count_{t} - 1$, and the size of the second is $\frac{t + 1 - count_{t+1}}{2}$ as expected.

With this property we can prove by contradiction that Boyer-Moore always return the majority element $m$ (if it exists).

Suppose that BM returns $m'$ which is not a majority element in $\Sigma \implies m'$ occurs $\leq\frac{n}{2}$ times. This would mean that at the end of the stream there are:

- $count_{n} \leq \frac{n}{2}$ instances of $m'$
- $\frac{n - count_{n}}{2}$ pairs $(e_{1}, e_{2})$ with $e_{1} \neq e_{2}$.

Now $m'$ would have to appear in all the $\frac{n-count_{n}}{2}$ pairs to be the final candidate. But this is impossible, since $count_{n} \geq 0 \implies \frac{n-count_{n}}{2} \leq \frac{n}{2}$, thus there would have to be at least $\frac{n}{2}$ instances of $m'$ for it to be returned by BM.

### Sampling

First let's define what a _sample_ is:
> Given a set $X$ of $n$ elements and an integer $m \in [1; n[$, an **m-sample** of $X$ is a random subset $S \subset X$ of size $m$, such that for each $x \in X$, we have $Pr(x \in S) = \frac{m}{n}$, that is the sample is **uniform**.

In the _offline setting_, an m-sample of $X$ can be easily computed by simply uniformly drawing $m$ elements from $X$ without replacement.

But in the _streaming setting_ this is not possible, especially in scenarios where streams are potentially unbounded and at every time step the random sample is required to be updated (e.g. sensor monitoring).

Observe that for sampling purposes we will assume that all the elements in $\Sigma$ are distinct, otherwise multiple occurrencies of the same element may appear in $S$.

Let's formally define the sampling problem:
> Given a (possibly unbounded) stream $\Sigma = x_{1}, x_{2}, \dots$ and an integer $m < |\Sigma|$, mantain for every $t \geq m$, an $m$-sample $S$ of the prefix $\Sigma_{t} = x_{1}, \dots, x_{t}$.

If $t$ was fixed we could simply obtain an m-sample of the time slots ${t_{1}, \dots, t_{m}}$, and then draw in each picked slot $x_{t_{i}}$.

But how can we mantain an m-sample for every $t$?

#### Reservoir Sampling

The idea is to first build a reservoir of $m$ elements, then when the reservoir is filled, start to replace elements in it with new incoming ones.

#### Algorithm
```pseudo
\begin{algorithm}
\caption{Reservoir Sampling Algorithm}
\begin{algorithmic}
	\State $S \gets \emptyset$
	\ForAll{$x_t \in \Sigma$}
		\If{$t \leq m$}
			\State $S \gets S \cup x_t$
        \ElseIf{ with probability $m/t$ do the following}
	        \State evict an element $x$ from $S$ chosen with uniform probability
			\State $S \gets S \cup x_t$
	    \EndIf
    \EndFor
\end{algorithmic}
\end{algorithm}
```

Note that the algorithm does not need the size of the stream, in fact it can be unbounded, since it operated entry-by-entry lazily.

**Example**
![[stream-ressampling-exa.png]]

#### Analysis

We will prove now both the correctness and the complexity of the algorithm:
> **Theorem**
> Let $\Sigma = x_{1}, x_{2}, \dots$. At any time $t \geq m$, the set $S$ maintained by the reservoir sampling algorithm is an $m$-sample of $\Sigma_{t} = x_{1}, \dots, x_{t}$.
> Moreover, the algorithm features $O(1)$ update and query time complexity.

**Proof**:

Let $S_{t} = S$ after processing $x_{1}, x_{2}, \dots, x_{t}$ elements. We now show by induction on $t \geq m$ that $S_{t}$ is an m-sample of $x_{1}, \dots, x_{t}$, namely that for every $i \in [1; t]$ we have $Pr(x_{i} \in S_{t}) = \frac{m}{t}$.

- **Base case**: $t = m$ trivial because all elements of $\Sigma_{t}$ are in $S_{t}$.
- **Inductive step**: Assume that the property holds for any time $t - 1$, that is:
	$$
	\forall i \in [i; t-1]\quad Pr(x_{i} \in S_{t-1}) = m / t-1
	$$
	Now we derive $Pr(x_{i} \in S_{t}) \forall i \in[i;t]$ considering the algorithm construction:
	- $Pr(x_{t} \in S_{t}) = \frac{m}{t}$ by construction (else branch)
	- Consider an arbitrary $x_{i}$ with $i < t$ that was already in the m-sample at $t-1$.
	Define the event $A =x_{t}\text{ is added to } S\text{ at time } t$. Clearly $Pr(A) = \frac{m}{t}$ and $Pr(\neg A) = 1 - \frac{m}{t}$.
	Then by the **law of total probabilities** we can write
	$$
	Pr(x_{i} \in S_{t}) = \underbrace{ Pr(x_{i} \in S_{t} \land A) }_{ (1) } + \underbrace{ Pr(x_{i} \in S_{t} \land \neg A) }_{ (2) }
	$$
	Now let's analyze the pieces: (1)
	$$
	\begin{align}
	Pr(x_{i} \in S_{t} \land A) = Pr(x_{i} \in S_{t} \mid A) \cdot \underbrace{ Pr(A) }_{ \frac{m}{t} } \\
	 Pr(x_{i} \in S_{t} \mid A) = \underbrace{ Pr(x_{i} \in S_{t-1}) }_{ m / (t-1)\text{ by induction} } \cdot \underbrace{ Pr(x_{i}\text{ not evicted at time }t) }_{ 1 - \frac{1}{m} } \\
	\implies Pr(x_{i} \in S_{t} \land A) = \frac{m}{t-1} \cdot \left( 1 - \frac{1}{m} \right)\cdot \frac{m}{t}
	\end{align}
	$$
	(2)
	$$
	\begin{align}
	Pr(x_{i} \in S_{t} \land \neg A) = Pr(x_{i} \in S_{t} \mid \neg A) \cdot \underbrace{ Pr(\neg A) }_{ 1-\frac{m}{t} } \\
	 Pr(x_{i} \in S_{t} \mid \neg A) = Pr(x_{i} \in S_{t-1}) = \frac{m}{t-1}\text{ by induction}  \\
	\implies Pr(x_{i} \in S_{t} \land \neg A) = \frac{m}{t-1} \cdot \left( 1 - \frac{m}{t} \right)
	\end{align}
	$$
	Now putting everything together:
	$$
\begin{align}
Pr(x_{i} \in S_{t}) & = (1) + (2) \\
 & =\frac{m}{t-1}\left( 1 - \frac{1}{m} \right) \frac{m}{t} + \frac{m}{t-1}\left( 1-\frac{m}{t} \right) \\
 & = \frac{m}{t}
\end{align}
$$

### Frequent Items

An application of the sampling technique can be seen in the _frequent items problem_.

> **Frequent Items Problem**
> Given a stream $\Sigma = x_{1}, \dots, x_{n}$ of $n$ items from a universe $U$, and a frequency threshold $\varphi \in (0, 1)$, determine all distinct items that occur at least $\varphi \cdot n$ times in $\Sigma$. We call these items **frequent items**.

**Example**: Let $n = 100$, $U = \{ A, B, C, D, E \}$
- $A: 30$ occurrencies
- $B: 30$ occurrencies
- $C: 20$ occurrencies
- $D: 10$ occurrencies
- $E: 10$ occurrencies

With $\varphi = 0.3$ the frequent items are $A, B$. In other words the frequent items are the ones we _expect to see_ $\varphi$-percent of the time.

We observe that the number of frequent items has to be $\leq \frac{1}{\varphi}$, because all the probabilities have to sum to 1.

The exact strategy would be to use a map (e.g. hash table) to count the occurrencies of every item, then at the end of the stream check which items have $\geq \varphi n$ occurrencies.
The problem with this approach is that potentially the universe could be very large and items could be very uniformly distributed $\implies$ the table would need to be almost as large as the universe $U \implies$ not efficient both in time and space for large streams.

#### Frequent Items with Reservoir Sampling

Frequent items can be approximated through [[#Reservoir Sampling]]:
1. Extract an m-sample $S$ from $\Sigma$ with reservoir sampling. Note that now the same element may occur multiple times in the sample.
2. Return the subset $S' \subseteq S$ of distinct items in the sample.

Now if $m \geq 1 / \varphi$, for any given frequent item $a$, we expect to find at least one copy of $a \in S$, hence $\in S'$.
However this in not guaranteed and some frequent items may be missed, and some infrequent items may appear.

#### $\epsilon$-Approximate Frequent Items

We want to estabilsh a stricter definition for our problem:
> **$\epsilon$-Approximate Frequent Items Problem**
> Given the stream $\Sigma = x_{1}, \dots, x_{n}$ of $n$ items, a frequency threshold $\varphi \in (0, 1)$, and an accuracy parameter $\epsilon \in (0, \varphi)$, return a set of distinct items that
> 1. Includes **all items occurring at least $\varphi \cdot n$ times** in $\Sigma$.
> 2. Contains **no item occurring less than $(\varphi - \epsilon) \cdot n$ times** in $\Sigma$.

This formulation we **avoid false negatives** (all frequent items are returned), but we **tolerate high-frequency false positives**.

**Example**: Let $n = 100$, $U = \{ A, B, C, D, E \}$
- $A: 30$ occurrencies
- $B: 30$ occurrencies
- $C: 20$ occurrencies
- $D: 10$ occurrencies
- $E: 10$ occurrencies

With $\varphi = 0.3, \epsilon = 0.1$ the frequent items are $A, B$; while the $\epsilon$-AFI are $A, B, C$.

#### Sticky Sampling

**Sticky sampling** provides a _probabilistic solution_ to the $\epsilon$-AFI problem.

The algorithm requires:
- A **confidence parameter** $\delta \in (0, 1)$
- An **hash table** $S$, whose entries are pairs $(x, f_{e}(x))$ where:
	- The _key_ $x$ is an item of the stream.
	- The _value_ $f_{e}(x)$ is a _lower bound_ to the number of occurrencies of $x$ seen so far.
- A **sampling rate** chosen to ensure (with probability $1 - \delta$) that the frequent items are in the sample.

Recall that we assume to know $n$.

The idea is essentially to reuse the "standard" offline technique based on the hash-map to store the counters, but to reduce its size by not considering all the elements, but only sampling some of them.

##### Algorithm
```pseudo
\begin{algorithm}
\caption{Sticky Sampling Algorithm}
\begin{algorithmic}
	\State $S \gets$ empty hash table
	\State $r \gets \log(1/(\delta\varphi))/\epsilon$
	\Comment{sampling rate $r/n$}
	\ForAll{$x_t \in \Sigma$}
		\If{$(x_t, f_e(x_t)) \in S$}
			\State $f_e(x_t) \gets f_e(x_t) + 1$
			\Comment{Always process elts already in the table because size would not change}
		\Else
			\State add $(x_t, 1)$ to $S$ with probability $r/n$
			\Comment{start tracking $x_t$}
        \EndIf
    \EndFor
	\Return all items $x \in S$ with $f_e(x) \geq (\varphi - \epsilon)n$
\end{algorithmic}
\end{algorithm}
```

##### Analysis
> **Theorem**
> Sticky sampling solves the $\epsilon$-AFI problem correctly with probability $\geq 1 - \delta$ and requires
> - _Working memory_ of size $O(r) = O\left( \ln\left( \frac{1}{\delta \varphi} \right) / \epsilon \right)$, in expectation
> - 1 pass
> - $O(1)$ expected update time
> - $O(r) = O\left( \log\left( \frac{1}{\delta \varphi} \right)/ \epsilon\right)$ expected query time

Notice that **the space is independent of the stream length**.

**Proof**:
- **Working memory size**: The working memory size depends on the number of entries in the hash table $S$ (assuming that an entry requires $O(1)$ space).
	Now, each item $x_{t}$ contributes a new entry to the hash table $S$ with probability $p_{t} \leq \frac{r}{n}$ ($\leq$ because the same element may already have been added).
	Let
	$$
	X_{t} = \begin{cases}
	1 & \text{if }x_{t}\text{ contributes a new entry to } S \\
	0 & \text{otherwise}
	\end{cases}
	$$
	At the end of the stream $|S| = \sum_{t=1}^{n}X_{t}$
	$$
\implies \mathbb{E}[|S|] = \mathbb{E}\left[ \sum_{t=1}^{n}X_{t}  \right] = \sum_{t=1}^{n} \mathbb{E}[X_{t}] = \sum_{t=1}^{n} p_{t} \leq n\cdot \frac{r}{n} = r
$$
- **1-pass, $O(1)$ update time**: Immediate from the algorithm construction and the properties of the hash tables.
- **Query time**: It requires iterating over all the entries in the hash table. From the analysis above we have seen that the size of $S$ is in the order of $O(r)$. The iterations take constant time (expected) $O(1) \implies$ the query time is $O(r)$.
- **Correctness**: We need to prove that with probability $\geq 1-\delta$ that the returned set of items satisfies [[#$epsilon$-Approximate Frequent Items|AFI]]:
	- (A): All the frequent items are in the set
	- (B): The set contains no item which occurs $< (\varphi - \epsilon)\cdot n$ times in $\Sigma$.
		
	Note that (B) is immediately ensured by the fact that we return only items whose counts in $S$ are $\geq (\varphi - \epsilon)\cdot n$, and we know that counts in $S$ are already lower bounds.

	Now let's prove (A):
	Consider a "true" frequent item $a$.
	$$
	Pr(a\text{ is not returned in the output}) \leq Pr(\text{none of the first } \lceil \epsilon n \rceil \text{ occurrences of } a \text{ in } \Sigma \text{ have been sampled})
	$$
	$$
\begin{align}
 & = \underbrace{ \left( 1-\frac{r}{n} \right)^{\lceil \epsilon n \rceil } }_{\text{indep. events}} \leq \left( 1-\frac{r}{n} \right)^{\epsilon n} =\\
 & =
\end{align}
$$

### Count-min Sketch

**Count-min** sketch is used to estimate the first (number of occurrencies) and second (number of occurrencies squared) frequency moment of the elements in a stream $\Sigma$.

#### Algorithm

```pseudo
\begin{algorithm}
\caption{Count-min Sketch Algorithm}
\begin{algorithmic}
	\State $C[j, k] \gets 0 \quad \forall j,k\ 0 \leq j < d, 0 \leq k < w$
	\ForAll{$x_t \in \Sigma$}
		\For{$0 \leq j \leq d - 1$}
			\State $C[j, h_j(x_t)] \gets C[j, h_j(x_t)] + 1$
        \EndFor
    \EndFor
\end{algorithmic}
\end{algorithm}
```

Then at the end the estimate for any $u \in U$ of its frequency $f_{u}$ is:
$$
\tilde{f}_{u} = \min_{0 \leq j \leq d-1}C[j, h_{j}(u)]
$$
Note that this estimate is **always overestimating** the value, because collisions can only increase the counter. This implies that the estimate is **biased** ($\mathbb{E}[\tilde{f}_{u}] > f_{u}$).

**Example**
![[stream-countminsketch-exa.png]]

#### Analysis

We will assume that:
1. For each $j \in [0; d-1]$ and each $u, v \in U$ with $u \neq v, h_{j}(u)$ and $h_{j}(v)$ are _independent random variables_ **uniformly distributed** in $[0; w - 1]$ (the "width" of the table, i.e. the number of counters).
2. The $d$ hash functions $h_{1}, \dots, h_{d}$ are **mutually independent**.

> **Theorem**
> Consider a $d \times w$ count-min sketch for a stream $\Sigma$ of length $n$, where $d = \log_{2}\left( \frac{1}{\delta} \right)$ and $w = \frac{2}{\epsilon}$, for some $\delta, \epsilon \in (0, 1)$. The sketch ensures that for any given $u \in U$ occurring in $\Sigma$.
> $$\tilde{f}_{u} - f_{u} \leq \epsilon\cdot n$$
> with probability $\geq 1 - \delta$.

Observe that:
- $\delta$ is related to the number of rows in the table, which represents intuitively the number of "tries" $\implies$ $\delta$ controls the **confidence** level in the estimate.
- $\epsilon$ is related to the number of columns in the table, which represents intuitively the number of "counters" used $\implies$ $\epsilon$ controls the **accuracy** of the estimate.
- The **bias** in the estimated frequencies ($\mathbb{E}[\tilde{f}] \neq f_{u}$) discourages their use to estimate the second moment, because it would further increase the bias in the estimate.

**Proof**
TODO

By [[Probability Cheat Sheet#Markov's Inequality|Markov's inequality]]:
$$
\begin{align}
  & Pr(C[j, j_{j}(u)] - f_{u} \geq 2\mathbb{E}[C[j, h_{j}(u)] - u]) \\
 \leq &  Pr\left( C[j, h_{j}(u)] - f_{u} \leq 2\cdot \frac{n\epsilon}{2} \right) \geq \frac{1}{2}  \\
 \implies  &  Pr(C[j, h_{j}(u)] - f_{u} > \epsilon n) \leq \frac{1}{2}
\end{align}
$$

Since $\tilde{f}_{u}$ is the _minimum_ estimate among all rows, so the minimum $C[j, h_{j}(u)]$ over all $0 \leq j \leq d - 1$, we have that:
$$
\begin{align}
Pr(\tilde{f}_{u} - f_{u} > \epsilon n)  = Pr(\forall 0 \leq j \leq d-1) C[h, h_{j}(u)] - f_{u} > \epsilon n) \\
= \prod_{j=0}^d{Pr(C[j, h_{j}(u)] - f_{u} > \epsilon n)} \leq \left( \frac{1}{2} \right)^{d} \\
\end{align}
$$

Since $d = \log_{2} 1/\delta$ we have that $Pr(\tilde{f}_{u} - f_{u} > \epsilon n) \leq \delta$.

### Count Sketch

The _count sketch_ is an **unbiased variant** of the [[#Count-min Sketch]].
The idea is that, while in the original version the contributions always gets added, now for each item $u \in U$ we randomly multiply the contribution to each row by either $+1$ or $-1$, so to try and _cancel out collisions_. The "inversion" is decided per element on every row.

#### Algorithm

```pseudo
\begin{algorithm}
\caption{Count Sketch Algorithm}
\begin{algorithmic}
\Require $d \times w$ array $C$ of counters
\Require $d$ hash functions $h_0, \dots, h_{d-1}$, with $h_j: U \mapsto \{0, \dots, w-1\} \forall j$
\Require $d$ hash functions $g_0, \dots, g_{d-1}$, with $g_j: U \mapsto \{-1, +1\} \forall j$
	\State $C[j, k] \gets 0 \quad \forall j,k\ 0 \leq j < d, 0 \leq k < w$
	\ForAll{$x_t \in \Sigma$}
		\For{$0 \leq j \leq d - 1$}
			\State $C[j, h_j(x_t)] \gets C[j, h_j(x_t)] + g_j(x_t)$
        \EndFor
    \EndFor
\end{algorithmic}
\end{algorithm}
```

Then at the end the estimate for any $u \in U$ of its frequency $f_{u}$ is:
$$
\tilde{f}_{u} = \text{median}_{0 \leq j \leq d-1}g_{j}(u)\cdot[j, h_{j}(u)]
$$

**Example**
![[stream-countmin-exa1.png]]
![[stream-countmin-exa2.png]]

#### Analysis

We assume that for both $h, g$ hash functions, the same assumptions of **independence and uniform distribution** made for the analysis of count-min sketch still hold.

> **Theorem**
> Consider a $d \times w$ count sketch for a stream $\Sigma$ of length $n$, where $d = \log_{2}\left( \frac{1}{\delta} \right)$ and $w = \frac{1}{\epsilon^{2}}$ for some $\delta, \epsilon \in (0, 1)$. The sketch ensures that for any given $u \in U$ occurring in $\Sigma$:
> 1. $\mathbb{E}[\tilde{f}_{u, j}] = f_{u}$, for any $j \in [0, d-1]$, i.e. $\tilde{f}_{u, j}$ is an **unbiased estimator**.
> 2. With probability $\geq 1 - \delta$
> $$ |\tilde{f}_{u} - f_{u}| \leq \epsilon \cdot \sqrt{ F_{2} }$$
> where $F_{2} = \sum_{u \in U}f_{u}^{2}$ (i.e. true second moment)

**Proof** (only point 1)

Fix $u \in U$ and $j \in [0; d-1]$ arbitrarily. We now show that $\mathbb{E}[\tilde{f}_{u, j}] = f_{u}$. Recall that $\tilde{f}_{j, u} = \underbrace{ g_{j}(u) }_{ \text{sign of hash} } \cdot C[j, h_{j}(u)]$.

For each item $a \in U, a \neq u$ (potential source of collisions with $u$), define the following $RV$, that represents the disturbance:
$$
Y_{a} = \begin{cases}
f_{a} & \text{if } h_{j}(a) = h_{j}(u) \text{ and } g_{j}(a) = g_{j}(u) \\
-f_{a} & \text{if } h_{j}(a) = h_{j}(u) \text{ and } g_{j}(a) = -g_{j}(u) \\
0 & \text{if } h_{j}(a) \neq h_{j}(u)
\end{cases}
$$
Thus $Y_{a}$ is the contribution of $a \neq u$ to the estimate $\tilde{f}_{j, u}$. We can write:
$$
\tilde{f}_{j, u} = f_{u} + \sum_{a \in U, a\neq u}Y_{a}
$$

Now for $a \neq u$:
$$
\begin{align}
Pr(Y_{a} = f_{a}) & = \underbrace{ Pr(h_{j}(a) = h_{j}(u)) }_{ \frac{1}{w} } \cdot \underbrace{ Pr(g_{j}(a) = g_{j}(u)) }_{ \frac{1}{2} }  & \text{(recall $u$ is fixed)} \\
 & = \frac{1}{2w} \\
 & = Pr(Y_{a} = -f_{a})  & \text{(by symmetry)}
\end{align}
$$
$$
Pr(Y_{a} = 0) = 1 - \frac{1}{w}\quad (= Pr(h_{j}(a) \neq h_{j}(u)))
$$

Then:
$$
\mathbb{E}[\tilde{f}_{j, u}] = \mathbb{E}[f_{u} + \Sigma_{a \in U, a\neq u}Y_{a}] = f_{u} + \mathbb{E}[\Sigma_{a \in U, a\neq u}Y_{a}] = f_{u} + \sum_{a \in U, a\neq u}\mathbb{E}[Y_{a}] 
$$
and:
$$
\begin{align}
\mathbb{E}[Y_{a}] =  &  f_{a} \cdot Pr(Y_{a} = f_{a}) \\
 &  - f_{a}\cdot Pr(Y_{a} - f_{a})  \\
 & + 0 \cdot Pr(Y_{a} = a) = f_{a} \frac{1}{2w} - f_{a} \cdot \frac{1}{2w} + 0 = 0 \\
\implies \mathbb{E}[\tilde{f}_{u, j}] = f_{u}
\end{align}
$$

As a consequence of this theorem the estimate obtained by count sketch is **suitable for estimating $F_{2}$**.

### $\tilde{F}_{2}$ Estimation

Given the previous [[#Count Sketch]] result we can obtain easily an estimate for the second moment:
$$
\tilde{F}_{2, j} = \sum_{k = 0}^{w - 1}C[j, k]^{2} \quad \text{for } 0 \leq j \leq d-1
$$

Then the estimate for the true second moment is:
$$
\tilde{F}_{2} = \text{median of the } \tilde{F}_{2,j}
$$

#### Analysis

Assume the same assumptions made for count sketch.

> **Theorem**
> Consider a $d \times w$ count sketch for a stream $\Sigma$ of length $n$, where $d = \log_{2}\left( \frac{1}{\delta} \right)$ and $w = \frac{1}{\epsilon^{2}}$ for some $\delta, \epsilon \in (0, 1)$. The sketch ensures that for any given $u \in U$ occurring in $\Sigma$:
> 1. $\mathbb{E}[\tilde{F}_{2, j}] = F_{2}$, for any $j \in [0, d-1]$, i.e. $\tilde{F}_{2, j}$ is an **unbiased estimator**.
> 2. With probability $\geq 1 - \delta$
> $$ |\tilde{F}_{2} - F_{2}| \leq \epsilon \cdot \sqrt{ F_{2} }$$
> where $F_{2} = \sum_{u \in U}f_{u}^{2}$ (i.e. true second moment)

TODO: Prove point 1. again for the new estimation using the same "template" used for count sketch

### Performance of Count-Min and Count Sketches

Both count-min and count sketches can be computed in **1 pass**.

We assume that:
1. Each hash function can be computed in _constant time_ $O(1)$.
2. The space occupied by the sketch table _dominates_ the space required to store the hash functions.

For both sketches we have
- **Working memory**: comes from the table $O(d \cdot w)$, which becomes $O\left( \log_{2}\left( \frac{1}{\delta} \right) / \epsilon \right)$ for the count-min sketch, and $O\left( \log_{2}\left( \frac{1}{\delta} \right) / \epsilon^{2} \right)$, for the count sketch; in order to attain the probabilistic accuracy stated before.
- **Processing time per element**: comes from the rows used $O(d) = O\left( \log_{2}\left( \frac{1}{\delta} \right) \right)$.

Moreover, given the sketch, the estimates $\tilde{f}_{u}$ and $\tilde{F}_{2}$ can be computed in $O(d)$ and $O(d\cdot w)$ time, respectively.

### Count-Min Sketch vs Count Sketch

| | **count-min** | **count sketch** |
|-|-|-|
|**Bias**| Biased estimates (overestimating) | Unbiased estimates|
|**Accuracy**| $\tilde{f}_{u} - f_{u} \leq \epsilon n$ | $\text{mod}(\tilde{f}_{u} - f_{u}) \leq \epsilon \sqrt{ F_{2} }$

### Filtering

For many applications, processing a data stream $\Sigma = x_{1},, x_{2}, \dots$ involves (maybe as one of the steps), the _identification_ of the items $x_{i}$ that **meet a certain criterion**.

Some of them can be easily checked with minimum cost in both time and space. However, this is not always the case.

For instance suppose that the items $x_{i}$ are email addresses, and that we want to check whether the incoming addresses belong to a set $S$ of _verified addresses_. If the set $S$ is very large (e.g. 1 bilion of addresses), we will face some issues:
1. $S$ very large $\implies$ it cannot fit in main memory, thus it has to be **stored on disk**.
2. Standard techniques to check whether $x_{i} \in S$, especially considering $S$ is on disk, may be too time-costly, and infeasible to perform with items arriving very frequently.

#### Bloom Filter

Bloom filter is a technique used to check whether an element belongs to a subset of the universe $U$, $S \subset U$.
The algorithm has the properties of:
- **No false negatives** i.e. $x_{i} \in S \implies BF(x_{i})$ will return `True`.
- **Low probability of false positives** (i.e. $x_{i} \not\in S \implies BF(x_{i})$ may return `True` with very low probability).

Let's define more precisely the problem

> **Approximate Membership Problem**
> Given a stream $\Sigma = x_{1}, x_{2},\dots$ of elements from some universe $U$, and let $S \subset U$ with $|S| = m$. We want to store a _compact representation_ of $S$ (sketch), such that, for any given $x_{i} \in S$, allows to check $x_{i} \in S$ with:
> 1. No error when $x_{i} \in S$ (**No false negatives**)
> 2. Small probability of error when $x_{i} \not\in S$ (**Small false positive rate**)

##### Algorithm 

The main ingredients of the Bloom filter are:
- **Array $A$ of $n$ bits**, all initialized to 0.
- **$k$ hash functions**: $h_{0}, h_{1}, \dots, h_{k-1}$ with $$h_{j}: U \to \{ 0, 1, \dots , n-1 \} \quad\forall j \in [0; k-1]$$

The idea is to create "fingerprints" of the items in $S$, composed by positions in the array $A$ given by the hash functions. Then to check an item $x_{i}$ we compare its "fingerprint", if it matches with all 1s in $A$, then we return `True`.
There clearly may be collisions, because items $x_{i} \not\in S$ may have a matching fingerprint, but as we will show in the analysis the probability can be made arbitrarily small.
Note that there **cannot be any false negatives** by construction.

```pseudo
\begin{algorithm}
\caption{Bloom filter algorithm}
\begin{algorithmic}
\Comment{Initialization}
\ForAll{$e \in S$}
	\For{$0 \leq j < k$}
		\State $A[h_j(e)] \gets 1$
    \EndFor
\EndFor
\Comment{Membership test}
\ForAll{$x_i \in \Sigma$}
	\State $x_i \in S \iff A[h_0(x_i)] = A[h_1(x_i)] = \dots = A[h_{k - 1}(x_i)] = 1$	
\EndFor
\end{algorithmic}
\end{algorithm}
```

###### Properties
Let's reassume all the straightforward properties:
- There are **no false negatives**
- Assuming $k \ll n$, and that hash functions can be stored compactly, the required working memory is dominated by $A \implies O(n \text{ bits})$
- Assuming that every hash function can be computed in $O(1)$ time, the **membership test requires $O(k)$ time** (one per hash function).

**Example**
Let:
$$
\begin{align}
 & S = \{ X, Y, Z \} \\
 & n = 12 \\
 & k = 2 \\
 & \begin{array}{|c|c|c|c|}
 \hline
   & X & Y & Z \\
 \hline
h_{0} & 0 & 1 & 4 \\
h_{1} & 4 & 9 & 9 \\
 \hline
\end{array}
\end{align}
$$

Then the resulting array $A$ is:
$$
A \equiv \begin{array}{|c|c|}
\hline
 1&  1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\

\hline
\end{array}
$$

Now suppose that
- $x_{i} = X \implies A[h_{0}(X)] = A[0] = A[h_{1}(X)] = A[h_{2}(X)] = 1 \implies X \in S$ **True positive**
- $x_{i} = T \implies A[h_{0}(T)] = A[2] = 0  \implies T \in S$ **True negative**
- $x_{i} = W \implies A[h_{0}(W)] = A[0] = A[h_{1}(W)] = A[h_{2}(W)] = 1 \implies W \in S$ **False positive**

##### Analysis

Let's assume again that for all the hash functions $h_{j}$, their output are **independent and uniformly distributed**.

> **Theorem**
> Suppose that $n$ is sufficiently large. For any given $x_{i}$ which does not belong to $S$, the probability that $x_{i}$ is erroneously claimed to be in $S$ is:
> $$Pr(A[h_{j}(x_{i})] = 1) \quad \forall j \in [0; k-1] \simeq (1 - e^{-km/n})^{k}$$
> This probability is referred to as **false positive rate** (FPR)

**Proof**

After the initialization of the array, the indices of the cells of $A$ set to 1 can be seen as $k\cdot m$ iid RVs $\in [0; n -1]$, because of the assumptions made on the hash functions.

Now, after $A$ has been initialized, consider an arbitrary index $l \in [0;n-1]$.
The probability of that index being zero is:
$$
\begin{align}
Pr(A[l] = 0) &= \left(1 - \frac{1}{n}\right)^{k\cdot m} \\
 & =\left( 1-\frac{1}{n} \right)^{\frac{kmn}{n}} \\
 & =\left(\underbrace{ \left( 1-\frac{1}{n} \right)^{n} }_{ n \to \infty \text{ tends to } e^{-1} }\right)^{\frac{km}{n}} \\
 & = e^{-\frac{km}{n}}
\end{align}
$$

Now let $p = e^{-km/n}$. We assume for simplicity that $A$ contains _exactly_ $p\cdot n$ zeros. In fact $\mathbb{E}[\text{number of zeros in } A] = p\cdot n$.

Consider an element $x \in \Sigma$, $x \not\in S$.
Let $l_{j} = h_{j}(x) \quad \forall j \in [0; k-1]$, $l_{j}$ is uniformly distributed in the range $[0; n - 1]$ by the previous assumptions over $h_{j}$.
Therefore:
$$
\begin{align}
 & Pr(A[l_{j}] = 1) = 1 - \frac{\overbrace{ p\cdot n }^{ n\text{ of zeros} }}{n} = 1 - p  & \text{ (probability of NOT hitting a }0\text{)} \\
 & \implies Pr(A[l_{1}] = A[l_{2}] = \dots = A[l_{k}] = 1) = (1 - p)^{k} & \text{(RV independent)}
\end{align}
$$

So finally:
$$
FPR = (1-e^{-km/n})^{k}
$$

We observe that, given fixed $m, n$, the $FPR$ is **minimized** by picking:
$$
k = \frac{n}{m}\log(2)
$$
In this case:
$$
\begin{align}
FPR  & = \left( 1-e^{\frac{n}{m}\log(2)\frac{m}{n}} \right)^{k} \\
 & = \left( \frac{1}{2} \right)^{k} \\
 & = \left( \frac{1}{2} \right)^{\frac{n}{m}\log(2)} \\
 & \simeq (0.6185)^{n/m}
\end{align}
$$

##### Comparison with "naive" hashing

An alternative approach would be to use a hash table with $2^{b}$ entries ($b$ is the size in bits), and:
1. Map each $s \in S$ into an integer $\in[0; 2^{b-1}]$ using a hash function
2. Let $\gamma_{1}, \dots, \gamma_{n}$ be the "fingerprints" associated to $s_{1}, \dots, s_{n}$.
3. A new element $x \in \Sigma$ is claimed to be $\in S \iff Fp(x) = \gamma_{i}$ for some $i$

The size of the hash table would be $\theta(m\cdot b)$. It can be shown that the $FPR$ is lower bounded by $\log(m)$, while with Bloom filter we can obtain the same $FPR$ by using much less space.